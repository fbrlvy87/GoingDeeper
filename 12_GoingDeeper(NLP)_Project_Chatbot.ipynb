{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affecting-advertiser",
   "metadata": {},
   "source": [
    "지난 노드에서 __챗봇과 번역기는 같은 집안__이라고 했던 말을 기억하시나요?\n",
    "앞서 배운 Seq2seq번역기와 Transfomer번역기에 적용할 수도 있겠지만, 이번 노드에서 배운 번역기 성능 측정법을 챗봇에도 적용해봅시다. 배운 지식을 다양하게 활용할 수 있는 것도 중요한 능력이겠죠. 이번 프로젝트를 통해서 챗봇과 번역기가 같은 집안인지 확인해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "contrary-fiber",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:33:49.738465Z",
     "start_time": "2021-05-06T04:33:49.735393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-motorcycle",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-seeking",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-survivor",
   "metadata": {},
   "source": [
    "아래 링크에서 ChatbotData.csv 를 다운로드해 챗봇 훈련 데이터를 확보합니다. csv 파일을 읽는 데에는 pandas 라이브러리가 적합합니다. 읽어 온 데이터의 질문과 답변을 각각 questions, answers 변수에 나눠서 저장하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-reply",
   "metadata": {},
   "source": [
    "- [songys/Chatbot_data](songys/Chatbot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-scratch",
   "metadata": {},
   "source": [
    "## 데이터 로드 & 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "drawn-glossary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:33:57.994612Z",
     "start_time": "2021-05-06T04:33:57.966928Z"
    }
   },
   "outputs": [],
   "source": [
    "# data load\n",
    "path_to_file = os.getenv('HOME')+'/aiffel/songys/Chatbot_data/ChatbotData.csv'\n",
    "data = pd.read_csv(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "worse-incident",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:33:58.553436Z",
     "start_time": "2021-05-06T04:33:58.545814Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-letter",
   "metadata": {},
   "source": [
    "## 데이터의 질문과 답변 분할 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "obvious-familiar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:34:08.265476Z",
     "start_time": "2021-05-06T04:34:08.249986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  12시 땡! + 하루가 또 가네요.\n",
      ">>  1지망 학교 떨어졌어 + 위로해 드립니다.\n",
      ">>  3박4일 놀러가고 싶다 + 여행은 언제나 좋죠.\n",
      ">>  3박4일 정도 놀러가고 싶다 + 여행은 언제나 좋죠.\n",
      ">>  PPL 심하네 + 눈살이 찌푸려지죠.\n",
      ">>  SD카드 망가졌어 + 다시 새로 사는 게 마음 편해요.\n",
      ">>  SD카드 안돼 + 다시 새로 사는 게 마음 편해요.\n",
      ">>  SNS 맞팔 왜 안하지ㅠㅠ + 잘 모르고 있을 수도 있어요.\n",
      ">>  SNS 시간낭비인 거 아는데 매일 하는 중 + 시간을 정하고 해보세요.\n",
      ">>  SNS 시간낭비인데 자꾸 보게됨 + 시간을 정하고 해보세요.\n",
      ">>  SNS보면 나만 빼고 다 행복해보여 + 자랑하는 자리니까요.\n",
      ">>  가끔 궁금해 + 그 사람도 그럴 거예요.\n",
      ">>  가끔 뭐하는지 궁금해 + 그 사람도 그럴 거예요.\n",
      ">>  가끔은 혼자인게 좋다 + 혼자를 즐기세요.\n",
      ">>  가난한 자의 설움 + 돈은 다시 들어올 거예요.\n",
      ">>  가만 있어도 땀난다 + 땀을 식혀주세요.\n",
      ">>  가상화폐 쫄딱 망함 + 어서 잊고 새출발 하세요.\n",
      ">>  가스불 켜고 나갔어 + 빨리 집에 돌아가서 끄고 나오세요.\n",
      ">>  가스불 켜놓고 나온거 같아 + 빨리 집에 돌아가서 끄고 나오세요.\n",
      ">>  가스비 너무 많이 나왔다. + 다음 달에는 더 절약해봐요.\n"
     ]
    }
   ],
   "source": [
    "src = []\n",
    "tgt = []\n",
    "for s,t in zip(data['Q'],data['A']):\n",
    "    src.append(str(s))\n",
    "    tgt.append(str(t))\n",
    "\n",
    "for s,t in zip(src[:20],tgt[:20]):\n",
    "    print(\">> \", s, \"+\",t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-omega",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-profession",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-filling",
   "metadata": {},
   "source": [
    "아래 조건을 만족하는 preprocess_sentence() 함수를 구현하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-transmission",
   "metadata": {},
   "source": [
    "1. 영문자의 경우, __모두 소문자로 변환__합니다.\n",
    "2. 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 __정규식을 활용하여 모두 제거__합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-induction",
   "metadata": {},
   "source": [
    "_문장부호 양옆에 공백을 추가하는 등 이전과 다르게 생략된 기능들은 우리가 사용할 토크나이저가 지원하기 때문에 굳이 구현하지 않아도 괜찮습니다!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "assumed-destination",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:34:18.516214Z",
     "start_time": "2021-05-06T04:34:18.513216Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^0-9ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-kernel",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-parliament",
   "metadata": {},
   "source": [
    "# Step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-offense",
   "metadata": {},
   "source": [
    "토큰화에는 _KoNLPy_의 mecab 클래스를 사용합니다.\n",
    "\n",
    "아래 조건을 만족하는 build_corpus() 함수를 구현하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-insider",
   "metadata": {},
   "source": [
    "1. __소스 문장 데이터__와 __타겟 문장 데이터__를 입력으로 받습니다.\n",
    "2. 데이터를 앞서 정의한 preprocess_sentence() 함수로 __정제하고, 토큰화__합니다.\n",
    "3. 토큰화는 __전달받은 토크나이즈 함수를 사용__합니다. 이번엔 mecab.morphs 함수를 전달하시면 됩니다.\n",
    "4. 토큰의 개수가 일정 길이 이상인 문장은 __데이터에서 제외__합니다.\n",
    "5. __중복되는 문장은 데이터에서 제외__합니다. 소스 : 타겟 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사합니다. 중복 쌍이 흐트러지지 않도록 유의하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-pressure",
   "metadata": {},
   "source": [
    "구현한 함수를 활용하여 questions 와 answers 를 각각 que_corpus , ans_corpus 에 토큰화하여 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "boolean-subdivision",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:34:33.436229Z",
     "start_time": "2021-05-06T04:34:32.700961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2057, 209, 2581, 105] [264, 9, 136, 9, 39, 4]\n",
      "[284, 3559, 599, 1043, 13] [624, 17, 1492, 4]\n",
      "[279, 2058, 480, 65, 271, 261, 9, 11, 44, 35] [275, 16, 672, 10, 34, 4]\n",
      "[3560, 1126, 36] [5180, 5, 5181, 19, 34, 4]\n",
      "[3561, 1535, 3562, 13] [121, 2000, 188, 6, 24, 51, 5182, 4]\n",
      "[544, 190, 1044, 132, 42, 7, 19, 627] [57, 116, 11, 15, 8, 38, 23, 15, 30, 4]\n",
      "[544, 73, 1536, 102, 14, 21, 46, 412, 7, 6, 146] [73, 8, 2452, 11, 17, 18, 12, 4]\n",
      "[544, 18, 37, 22, 66, 1127, 11, 35, 152, 17, 521] [2864, 7, 6, 846, 254, 4]\n",
      "[494, 318, 17] [90, 27, 23, 217, 14, 28, 4]\n",
      "[494, 16, 180, 102, 24, 10, 35] [180, 31, 969, 12, 4]\n",
      "[3563, 33, 206, 40, 3564] [266, 16, 121, 3278, 14, 28, 4]\n",
      "[3565, 15, 239, 2059, 1757] [2059, 8, 3279, 56, 12, 4]\n",
      "[3566, 3567, 3568, 2582] [291, 123, 11, 513, 1162, 7, 12, 4]\n",
      "[2060, 1045, 1758, 11, 1537, 13] [523, 228, 26, 844, 143, 982, 11, 562, 12, 4]\n",
      "[2060, 325, 54, 78, 825, 35, 4] [424, 140, 26, 6, 67, 1873, 17, 49, 4]\n",
      "[2060, 325, 3569, 863, 1128, 25, 13] [608, 7, 24, 707, 105]\n",
      "[454, 436, 33, 81, 534, 20] [454, 436, 33, 73, 16, 106, 5, 84, 4, 380, 167, 305, 8, 405, 11, 181, 7, 1067, 73, 8, 1536, 7, 19, 113, 4]\n",
      "[600, 275, 9, 62, 93, 59, 13] [1075, 600, 5, 445, 51, 26, 367, 6, 281, 76, 9, 18, 12, 4]\n",
      "[600, 15, 13, 20] [99, 31, 243, 13, 550, 27, 8, 429, 322, 230, 99, 129, 208, 17, 56, 6, 27, 8, 45, 93, 47, 7, 11, 15, 30]\n",
      "[600, 1538, 275, 783, 4] [67, 3280, 790, 9, 55, 25, 39, 4]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "vocab_size = 20000\n",
    "tokenizer = Mecab()\n",
    "max_len = 50\n",
    "\n",
    "def build_corpus(src, tgt, l, num_words=vocab_size, dup=0):\n",
    "    if dup == 0:\n",
    "        sen_idx = {}\n",
    "        src_u = []\n",
    "        tgt_u = []\n",
    "\n",
    "        for sen1, sen2 in zip(src, tgt):\n",
    "            if sen1 not in sen_idx:\n",
    "                sen_idx[sen1] = 1\n",
    "                src_u.append(sen1)\n",
    "                tgt_u.append(sen2)\n",
    "\n",
    "        sen_idx = {}\n",
    "        src = []\n",
    "        tgt = []\n",
    "\n",
    "        for sen1, sen2 in zip(src_u, tgt_u):\n",
    "            if sen2 not in sen_idx:\n",
    "                sen_idx[sen2] = 1\n",
    "                src.append(sen1)\n",
    "                tgt.append(sen2)\n",
    "\n",
    "    src_p = []\n",
    "    tgt_p = []\n",
    "    \n",
    "    for s, t in zip(src, tgt):\n",
    "        src_p.append(preprocess_sentence(s))\n",
    "        tgt_p.append(preprocess_sentence(t))\n",
    "\n",
    "    src_tok = []\n",
    "    tgt_tok = []\n",
    "    word_tok = []\n",
    "\n",
    "    for s in src_p:\n",
    "        tmp = tokenizer.morphs(s)\n",
    "        src_tok.append(tmp)\n",
    "        word_tok.append(tmp)\n",
    "\n",
    "    for t in tgt_p:\n",
    "        tmp = tokenizer.morphs(t)\n",
    "        tgt_tok.append(tmp)\n",
    "        word_tok.append(tmp)\n",
    "\n",
    "    words = np.concatenate(word_tok).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(num_words-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<EOS>'] + [key for key, _ in counter]\n",
    "    \n",
    "    # 사전 구성\n",
    "    word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "    # 변환 text to index\n",
    "    src_data = list(map(wordlist_to_indexlist, src_tok))\n",
    "    tgt_data = list(map(wordlist_to_indexlist, tgt_tok))\n",
    "\n",
    "    src_l = []\n",
    "    tgt_l = []\n",
    "\n",
    "    for s, t in zip(src_data, tgt_data):\n",
    "        if len(s) <= l and len(t) <= l:\n",
    "            src_l.append(s)\n",
    "            tgt_l.append(t)\n",
    "\n",
    "    return src_l, tgt_l, word_to_index\n",
    "\n",
    "\n",
    "que_corpus, ans_corpus, word_to_index = build_corpus(src, tgt, max_len)\n",
    "for q, a in zip(que_corpus[:20], ans_corpus[:20]):\n",
    "    print(q, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-reference",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-shopping",
   "metadata": {},
   "source": [
    "# Step 4. Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-excitement",
   "metadata": {},
   "source": [
    "우리에게 주어진 데이터는 __1만 개가량으로 적은 편__에 속합니다. 이럴 때에 사용할 수 있는 테크닉을 배웠으니 활용해봐야겠죠? __Lexical Substitution을 실제로 적용__해보도록 하겠습니다.\n",
    "\n",
    "아래 링크를 참고하여 __한국어로 사전 훈련된 Embedding 모델을 다운로드__합니다. Korean (w) 가 Word2Vec으로 학습한 모델이며 용량도 적당하므로 사이트에서 Korean (w)를 찾아 다운로드하고, ko.bin 파일을 얻으세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-lottery",
   "metadata": {},
   "source": [
    "- [Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-blogger",
   "metadata": {},
   "source": [
    "다운로드한 모델을 활용해 데이터를 __Augmentation__ 하세요! 앞서 정의한 lexical_sub() 함수를 참고하면 도움이 많이 될 겁니다.\n",
    "\n",
    "_Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록, 이후엔 반대로 원본 que_corpus 와 Augmentation된 ans_corpus 가 병렬을 이루도록 하여 __전체 데이터가 원래의 3배가량으로 늘어나도록__ 합니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "informal-rover",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:42:55.946069Z",
     "start_time": "2021-05-06T04:42:55.710323Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "wv = gensim.models.Word2Vec.load('~/aiffel/songys/ko/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "pleasant-probe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:43:06.171452Z",
     "start_time": "2021-05-06T04:43:06.168248Z"
    }
   },
   "outputs": [],
   "source": [
    "def lexical_sub(sentence, word2vec, top=0):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence.split()\n",
    "    \n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[top][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-fraud",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:44:35.361206Z",
     "start_time": "2021-05-06T04:43:56.612152Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "for idx in tqdm_notebook(range(7731)):\n",
    "    \n",
    "    old_src = []\n",
    "    for w in que_corpus[idx]:\n",
    "        ow = index_to_word[w]\n",
    "        old_src.append(ow)\n",
    "    old_src = ' '.join(old_src)\n",
    "\n",
    "    old_tgt = []\n",
    "    for w in ans_corpus[idx]:\n",
    "        ow = index_to_word[w]\n",
    "        old_tgt.append(ow)\n",
    "    old_tgt = ' '.join(old_tgt)\n",
    "    \n",
    "    new_src = [None]*3\n",
    "    new_tgt = [None]*3\n",
    "    \n",
    "    new_src[0] = old_src\n",
    "    new_tgt[0] = old_tgt\n",
    "    \n",
    "    new_src[1] = lexical_sub(old_src, wv)\n",
    "    new_src[2] = lexical_sub(old_src, wv, 1)\n",
    "    \n",
    "    new_tgt[1] = lexical_sub(old_tgt, wv)\n",
    "    new_tgt[2] = lexical_sub(old_tgt, wv, 1)  \n",
    "    \n",
    "    for i in new_src:\n",
    "        for j in new_tgt:\n",
    "            if i is not None and j is not None:\n",
    "                src_corpus.append(i)\n",
    "                tgt_corpus.append(j)\n",
    "    \n",
    "    \n",
    "print(src_corpus[:20])\n",
    "print(src[:20])\n",
    "print(len(src_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-peripheral",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-registrar",
   "metadata": {},
   "source": [
    "# Step 5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-router",
   "metadata": {},
   "source": [
    "타겟 데이터인 ans_corpus 에 \\<start> 토큰과 \\<end> 토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행합니다. 우리가 구축한 ans_corpus 는 list 형태이기 때문에 아주 쉽게 이를 해결할 수 있답니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-basin",
   "metadata": {},
   "source": [
    "``` python\n",
    "sample_data = [\"12\", \"시\", \"땡\", \"!\"]\n",
    "\n",
    "print([\"<start>\"] + sample_data + [\"<end>\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-badge",
   "metadata": {},
   "source": [
    "1. 위 소스를 참고하여 타겟 데이터 전체에 \\<start> 토큰과 \\<end> 토큰을 추가해 주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-diameter",
   "metadata": {},
   "source": [
    "챗봇 훈련 데이터의 가장 큰 특징 중 하나라고 하자면 바로 __소스 데이터와 타겟 데이터가 같은 언어를 사용한다는 것__이겠죠. 앞서 배운 것처럼 이는 Embedding 층을 공유했을 때 많은 이점을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-homework",
   "metadata": {},
   "source": [
    "2. 특수 토큰을 더함으로써 ans_corpus 또한 완성이 되었으니, que_corpus 와 결합하여 __전체 데이터에 대한 단어 사전을 구축__하고 __벡터화하여 enc_train 과 dec_train__ 을 얻으세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "progressive-leader",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:46:17.544578Z",
     "start_time": "2021-05-06T04:46:11.629366Z"
    }
   },
   "outputs": [],
   "source": [
    "new_que_corpus, new_ans_corpus, word_to_index = build_corpus(src_corpus,tgt_corpus,max_len,dup=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "educated-bridges",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:46:39.802233Z",
     "start_time": "2021-05-06T04:46:39.799774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58214\n",
      "58214\n"
     ]
    }
   ],
   "source": [
    "print(len(new_que_corpus))\n",
    "print(len(new_ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "massive-supervision",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:47:06.251356Z",
     "start_time": "2021-05-06T04:47:06.248177Z"
    }
   },
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "technical-syracuse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:47:16.093497Z",
     "start_time": "2021-05-06T04:47:16.064797Z"
    }
   },
   "outputs": [],
   "source": [
    "ans = []\n",
    "for a in new_ans_corpus:\n",
    "    ans.append([word_to_index[\"<BOS>\"]] + a + [word_to_index[\"<EOS>\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "presidential-dress",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:47:25.654187Z",
     "start_time": "2021-05-06T04:47:25.229059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_train : 57631 enc_val : 583\n",
      "dec_train : 57631 dec_val : 583\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(new_que_corpus, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(ans, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01)\n",
    "\n",
    "print(\"enc_train :\", len(enc_train), \"enc_val :\", len(enc_val))\n",
    "print(\"dec_train :\", len(dec_train), \"dec_val :\",len(dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "lesbian-first",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:47:53.518741Z",
     "start_time": "2021-05-06T04:47:53.516332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(enc_train[0]))\n",
    "print(len(dec_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-poison",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-probability",
   "metadata": {},
   "source": [
    "# Step 6. 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-guitar",
   "metadata": {},
   "source": [
    "앞서 번역 모델을 훈련하며 정의한 Transformer 를 그대로 사용하시면 됩니다! 대신 데이터의 크기가 작으니 하이퍼파라미터를 튜닝해야 과적합을 피할 수 있습니다. 모델을 훈련하고 아래 예문에 대한 답변을 생성하세요! __가장 멋진 답변__과 __모델의 하이퍼파라미터__를 제출하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-passing",
   "metadata": {},
   "source": [
    "``` python\n",
    "# 예문\n",
    "1. 지루하다, 놀러가고 싶어.\n",
    "2. 오늘 일찍 일어났더니 피곤하다.\n",
    "3. 간만에 여자친구랑 데이트 하기로 했어.\n",
    "4. 집에 있는다는 소리야.\n",
    "\n",
    "---\n",
    "\n",
    "# 제출\n",
    "\n",
    "Translations\n",
    "> 1. 잠깐 쉬 어도 돼요 . <end>\n",
    "> 2. 맛난 거 드세요 . <end>\n",
    "> 3. 떨리 겠 죠 . <end>\n",
    "> 4. 좋 아 하 면 그럴 수 있 어요 . <end>\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 1\n",
    "> d_model: 368\n",
    "> n_heads: 8\n",
    "> d_ff: 1024\n",
    "> dropout: 0.2\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 1000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "offshore-window",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:48:07.998329Z",
     "start_time": "2021-05-06T04:48:07.994320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "mounted-factor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:49:58.152761Z",
     "start_time": "2021-05-06T04:49:58.147537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Mask  생성하기\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "headed-andrews",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:50:07.507528Z",
     "start_time": "2021-05-06T04:50:07.498952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Multi Head Attention 구현\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "restricted-pitch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:50:13.438451Z",
     "start_time": "2021-05-06T04:50:13.434736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "clean-naples",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:50:31.145165Z",
     "start_time": "2021-05-06T04:50:31.140087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder의 레이어 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "level-wiring",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:50:41.543417Z",
     "start_time": "2021-05-06T04:50:41.537294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "entertaining-columbus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:50:55.932020Z",
     "start_time": "2021-05-06T04:50:55.927321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "pediatric-sigma",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:51:02.182646Z",
     "start_time": "2021-05-06T04:51:02.178108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "pressed-garage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:51:11.245374Z",
     "start_time": "2021-05-06T04:51:11.237375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "sized-justice",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:52:48.102856Z",
     "start_time": "2021-05-06T04:52:47.103125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=128,\n",
    "    n_heads=8,\n",
    "    d_ff=256,\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    pos_len=42,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 128\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "processed-bangkok",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:52:59.924005Z",
     "start_time": "2021-05-06T04:52:59.920034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate Scheduler 구현\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "coordinated-pipeline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:53:11.032076Z",
     "start_time": "2021-05-06T04:53:11.029186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "durable-lafayette",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:53:14.918876Z",
     "start_time": "2021-05-06T04:53:14.915298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "social-pottery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:53:26.323491Z",
     "start_time": "2021-05-06T04:53:26.319114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "civic-librarian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:57:13.257272Z",
     "start_time": "2021-05-06T04:53:54.775676Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj24/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2c71bd55ce4a79a1dda671f773ea0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/901 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0b742957a94c44815da533e4adf965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/901 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bd8d236c884a5d95e0751562de3261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/901 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc2b67f5d334cba918d5ac934954fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/901 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443c6a57a296419c81b5a9acfc67b134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/901 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련시키기\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-bloom",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-uniform",
   "metadata": {},
   "source": [
    "# Step 7. 성능 측정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-screen",
   "metadata": {},
   "source": [
    "챗봇의 경우, 올바른 대답을 하는지가 중요한 평가지표입니다. 올바른 답변을 하는지 눈으로 확인할 수 있겠지만, 많은 데이터의 경우는 모든 결과를 확인할 수 없을 것입니다. 주어잔 질문에 적절한 답변을 하는지 확인하고, BLEU Score를 계산하는 calculate_bleu() 함수도 적용해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "sudden-toyota",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:58:13.083413Z",
     "start_time": "2021-05-06T04:58:12.897215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj24/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/aiffel-dj24/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk # nltk가 설치되어 있지 않은 경우 주석 해제\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "spanish-injection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:58:33.916530Z",
     "start_time": "2021-05-06T04:58:33.911715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj24/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/aiffel-dj24/anaconda3/envs/aiffel/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "offshore-jacket",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T04:59:20.167345Z",
     "start_time": "2021-05-06T04:59:20.161762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "radio-danish",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:00:58.552968Z",
     "start_time": "2021-05-06T05:00:58.546029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# translate()\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    print(sentence)\n",
    "    s = src_tokenizer.morphs(sentence)\n",
    "    tokens = []\n",
    "    for i in s:\n",
    "        tokens.append(word_to_index[i])\n",
    "    pieces = tokens        \n",
    "        \n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([1], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if predicted_id == 3:\n",
    "            #result = tgt_tokenizer.decode_ids(ids)\n",
    "            result = []\n",
    "            for i in ids:\n",
    "                result.append(index_to_word[i])\n",
    "            print(result)\n",
    "            \n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    #result = tgt_tokenizer.decode_ids(ids)\n",
    "    result = ''\n",
    "    for i in ids:\n",
    "        result += index_to_word[i]\n",
    "    \n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    return result\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "individual-mystery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:01:13.778761Z",
     "start_time": "2021-05-06T05:01:13.771613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "\n",
    "        src_sentence = []\n",
    "        tgt_sentence = []\n",
    "\n",
    "        for w in src_tokens:\n",
    "            if w != 0:\n",
    "                ow = index_to_word[w]\n",
    "                src_sentence.append(ow)\n",
    "\n",
    "        src_sentence = ' '.join(src_sentence)\n",
    "\n",
    "        for w in tgt_tokens:\n",
    "            if w != 0 and w != 1 and w != 3:\n",
    "                ow = index_to_word[w]\n",
    "                tgt_sentence.append(ow)\n",
    "\n",
    "        tgt_sentence = ' '.join(tgt_sentence)\n",
    "\n",
    "        reference = preprocess_sentence(tgt_sentence).split()\n",
    "        candidate = translate(src_sentence, transformer, tokenizer, tokenizer)\n",
    "\n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)\n",
    "\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "digital-break",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:01:31.108831Z",
     "start_time": "2021-05-06T05:01:29.192862Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj24/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7abf83cbe944f50b08eb8410085f251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "싸운다 의 기준 그러 뭘까 ?\n",
      "['감정', '이', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '지요', '.']\n",
      "Source Sentence:  싸운다 의 기준 그러 뭘까 ?\n",
      "Model Prediction:  ['감정', '이', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '지요', '.']\n",
      "Real:  ['감정', '이', '상', '하', '면', '싸우', '는', '거', '죠', '.']\n",
      "Score: 0.021239\n",
      "\n",
      "아빠 놀 드 담배 그만 피웠으면\n",
      "['금연', '프로그램', '을', '소개', '해', '보', '세요', '.']\n",
      "Source Sentence:  아빠 놀 드 담배 그만 피웠으면\n",
      "Model Prediction:  ['금연', '프로그램', '을', '소개', '해', '보', '세요', '.']\n",
      "Real:  ['금연', '프로그램', '을', '소개', '해', '보', '세요', '.']\n",
      "Score: 1.000000\n",
      "\n",
      "관심 있 는 여자 애 에게 의미 있 는 선물 을 주 고 싶 은데 뭘 주 면 좋아할까요 ?\n",
      "['필요', '한', '게', '뭔지', '보', '세요', '.']\n",
      "Source Sentence:  관심 있 는 여자 애 에게 의미 있 는 선물 을 주 고 싶 은데 뭘 주 면 좋아할까요 ?\n",
      "Model Prediction:  ['필요', '한', '게', '뭔지', '보', '세요', '.']\n",
      "Real:  ['필요', '한', '게', '뭔지', '살펴보', '세요', '.']\n",
      "Score: 0.488923\n",
      "\n",
      "Num of Sample: 3\n",
      "Total Score: 0.5033874573204027\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[:3], dec_val[:3], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "champion-entertainment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:02:53.357176Z",
     "start_time": "2021-05-06T05:02:45.349073Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj24/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4574c6790804cecba2385bb244eb335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "싸운다 의 기준 그러 뭘까 ?\n",
      "['감정', '이', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '면', '싸우', '지요', '.']\n",
      "술 먹 고 카드 긁 었 어\n",
      "['술', '이', '웬수', '예요', '.']\n",
      "친한 연인 의 구 여친 이랑 사귀 어도 되 나 .\n",
      "['친구', '라면', '많이', '실망', '할', '거', '라', '생각', '해요', '.']\n",
      "유명 인 되 고 싶 어\n",
      "['저', '도', '괜찮', '아', '오', '세요', '.']\n",
      "여자 친구 가 변해 가 는 듯 같 아 .\n",
      "['모습', '간혹', '보', '는', '건', '어떨까', '요', '.']\n",
      "짝 이듬해 관련 돼서 악몽 을 자주 꾸 는데 불안 함 .\n",
      "['악몽', '되', '긴', '하', '겠', '네요', '.']\n",
      "눈 그러 계속 뻑뻑 해\n",
      "['인공', '눈물', '눈물', '인', '넣', '어', '졌', '죠', '.']\n",
      "사랑 하 는 사람 이랑 헤어져야 해\n",
      "['정말', '바라', '면', '돼요', '.']\n",
      "짝사랑 시작 하 는 데 감정 기복 이 심해 .\n",
      "['짝사랑', '이', '그런', '건가', '봐요', '.']\n",
      "이 별후 동침 그리고\n",
      "['달라진', '게', '많', '았', '겠', '네요', '.']\n",
      "이 길 이 니 에게 맞 는 걸까\n",
      "['잘', '가', '고', '있', '었', '을', '거', '예요', '.']\n",
      "어떤 남자 애 가 내일 . . 하 냐는데 나 한테 관심 있 나 ?\n",
      "['시간', '하', '고', '있', '는지', '찾아보', '세요', '.']\n",
      "일 하 는 이곳 근처 가 서 염탐 하 고 왔 네 .\n",
      "['그분', '에', '이유', '가', '필요', '한가요', '.']\n",
      "몸값 다 없 어 졌 어\n",
      "['돈', '있', '는', '있', '어요', '.']\n",
      "휴양지 가 는데 싶 다\n",
      "['저', '도', '쉬', '고', '싶', '어요', '.']\n",
      "궁금 한 듯 있 어 ! !\n",
      "['네', '무엇', '이', '든', '물', '어', '보', '세요', '.']\n",
      "이제 점심 시간 에 도 너 에게 카톡 을 할 수 가 없 구나 .\n",
      "['점심', '수', '있', '는', '익숙', '해', '봐요', '.']\n",
      "이 명절 을 어찌 보낼지 는데 허허\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [1,43,128] vs. [1,42,128] [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-b26e53f27804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-820d1f8ccf3f>\u001b[0m in \u001b[0;36meval_bleu\u001b[0;34m(src_corpus, tgt_corpus, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mreference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mcandidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         score = sentence_bleu([reference], candidate,\n",
      "\u001b[0;32m<ipython-input-63-33131f292b2e>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence, model, src_tokenizer, tgt_tokenizer)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mpieces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_attns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_attns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_enc_attns\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-33131f292b2e>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence, model, src_tokenizer, tgt_tokenizer)\u001b[0m\n\u001b[1;32m     29\u001b[0m               \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m               \u001b[0mcombined_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m               dec_padding_mask)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mpredicted_id\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-044c72de2cae>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausality_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0menc_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mdec_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_attns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-044c72de2cae>\u001b[0m in \u001b[0;36membedding\u001b[0;34m(self, emb, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_fc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    470\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [1,43,128] vs. [1,42,128] [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "eval_bleu(enc_val[::10], dec_val[::10], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "sharp-entry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:04:45.361065Z",
     "start_time": "2021-05-06T05:04:44.769520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지루하다 , 놀러가고 싶어 .\n",
      "['힘내', '지', '않', '을까', '하', '면', '후', '에', '게', '좋', '을', '거', '예요', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['힘내', '지', '않', '을까', '하', '면', '후', '에', '게', '좋', '을', '거', '예요', '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"지루하다, 놀러가고 싶어.\", transformer, tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "chicken-change",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:04:55.577444Z",
     "start_time": "2021-05-06T05:04:55.273718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 일찍 일어났더니 피곤하다 .\n",
      "['피', '작', '은', '후련', '하', '네요', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['피', '작', '은', '후련', '하', '네요', '.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"오늘 일찍 일어났더니 피곤하다.\", transformer, tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "helpful-contributor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:05:02.579791Z",
     "start_time": "2021-05-06T05:05:02.292753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "간만에 여자친구랑 데이트 하기로 했어 .\n",
      "['득템', '하', '길', '바라', '요', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['득템', '하', '길', '바라', '요', '.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"간만에 여자친구랑 데이트 하기로 했어.\", transformer, tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "martial-temperature",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:05:09.996607Z",
     "start_time": "2021-05-06T05:05:09.726530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "집에 있는다는 소리야 .\n",
      "['어떻게', '하', '면서', '면', '돼요', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['어떻게', '하', '면서', '면', '돼요', '.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"집에 있는다는 소리야.\", transformer, tokenizer, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-shoot",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-identity",
   "metadata": {},
   "source": [
    "__1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?__  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-hawaiian",
   "metadata": {},
   "source": [
    "- 챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-average",
   "metadata": {},
   "source": [
    "__2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?__  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-republican",
   "metadata": {},
   "source": [
    "- 과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-currency",
   "metadata": {},
   "source": [
    "__3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?__  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-teddy",
   "metadata": {},
   "source": [
    "- 주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-marshall",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-parameter",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-rebound",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
