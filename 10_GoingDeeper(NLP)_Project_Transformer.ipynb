{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "placed-director",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 다운로드 (로컬 유저용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-minneapolis",
   "metadata": {},
   "source": [
    "아래 링크에서 korean-english-park.train.tar.gz 를 다운로드받아 한영 병렬 데이터를 확보합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-continent",
   "metadata": {},
   "source": [
    "- [jungyeul/korean-parallel-corpora](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-chess",
   "metadata": {},
   "source": [
    "_💡이전 [ Seq2seq으로 번역기 만들기 ] 코스에서 사용한 데이터와 동일한 데이터입니다!_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-interstate",
   "metadata": {},
   "source": [
    "터미널을 열어서 하단의 명령어를 입력해주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-separation",
   "metadata": {},
   "source": [
    "```\n",
    "$ mkdir -p ~/aiffel/transformer/data\n",
    "$ cd ~/aiffel/transformer/data\n",
    "\n",
    "$ wget https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz\n",
    "$ gzip -d korean-english-park.train.tar.gz\n",
    "$ tar -xvf korean-english-park.train.tar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imposed-gibson",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:29:05.743765Z",
     "start_time": "2021-05-04T10:29:02.955287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import sentencepiece as spm\n",
    "import seaborn # Attention 시각화를 위해 필요!\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "associate-cambridge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:29:06.462918Z",
     "start_time": "2021-05-04T10:29:05.753728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-content",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-amateur",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 정제 및 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-thesaurus",
   "metadata": {},
   "source": [
    "1) set 데이터형이 __중복을 허용하지 않는다는 것을 활용__해 중복된 데이터를 제거하도록 합니다. 데이터의 __병렬 쌍이 흐트러지지 않게 주의__하세요! 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-pressing",
   "metadata": {},
   "source": [
    "## 데이터 로드 및 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "painful-softball",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:29:20.709357Z",
     "start_time": "2021-05-04T10:29:20.513820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78968"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    cleaned_corpus = list(set(zip(kor, eng)))\n",
    "\n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)\n",
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "racial-flight",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:29:44.742549Z",
     "start_time": "2021-05-04T10:29:44.732637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('그들의 해악적 유머는 청중들이 느낄 수 있는 많은 얘기를 하며 그들에게 즐거움을 주고 있다.',\n",
       " 'Their biting humor is something to which many in their audience can relate.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-meter",
   "metadata": {},
   "source": [
    "2) 정제 함수를 아래 조건을 만족하게 정의하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-graduation",
   "metadata": {},
   "source": [
    "## 정제 함수 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-humor",
   "metadata": {},
   "source": [
    ">___조건___\n",
    ">- _모든 입력을 ___소문자로 변환___합니다._\n",
    ">- ___알파벳, 문장부호, 한글___ _만 남기고 모두 제거합니다._\n",
    ">- ___문장부호 양옆에 공백___ _을 추가합니다._\n",
    ">- _문장 앞뒤의 ___불필요한 공백을 제거___합니다._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "capital-scanning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:34:31.301140Z",
     "start_time": "2021-05-04T10:34:31.289330Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Zㄱ-하-ㅣ가-힣0-9?.!,]+\", \" \", sentence) \n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-message",
   "metadata": {},
   "source": [
    "3) 한글 말뭉치 kor_corpus 와 영문 말뭉치 eng_corpus 를 각각 분리한 후, __정제하여 토큰화를 진행__합니다! __토큰화에는 Sentencepiece를 활용__하세요. 첨부된 공식 사이트를 참고해 아래 조건을 만족하는 generate_tokenizer() 함수를 정의합니다. 최종적으로 ko_tokenizer 과 en_tokenizer 를 얻으세요. en_tokenizer에는 set_encode_extra_options(\"bos:eos\") 함수를 실행해 __타겟 입력이 문장의 시작 토큰과 끝 토큰을 포함할 수 있게__ 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-evolution",
   "metadata": {},
   "source": [
    "> ___조건___\n",
    ">- _단어 사전을 매개변수로 받아 ___원하는 크기의 사전을 정의___할 수 있게 합니다. (기본: 20,000)_\n",
    ">- _학습 후 저장된 model 파일을 SentencePieceProcessor() 클래스에 Load()한 후 반환합니다._\n",
    ">- ___특수 토큰의 인덱스___ _를 아래와 동일하게 지정합니다._  \n",
    "_\\<PAD> : 0 / \\<BOS> : 1 /  \n",
    "\\<EOS> : 2 / \\<UNK> : 3_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-modem",
   "metadata": {},
   "source": [
    "- 참고: [google/sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-allowance",
   "metadata": {},
   "source": [
    "## 데이터 분할 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "timely-customs",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:34:36.972670Z",
     "start_time": "2021-05-04T10:34:34.163420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78968\n",
      "78968\n"
     ]
    }
   ],
   "source": [
    "kor_corpus = []\n",
    "eng_corpus = []\n",
    "\n",
    "for sentence in cleaned_corpus:\n",
    "    ko_sentence = preprocess_sentence(sentence[0])\n",
    "    en_sentence = preprocess_sentence(sentence[1])\n",
    "    kor_corpus.append(ko_sentence)\n",
    "    eng_corpus.append(en_sentence)   \n",
    "    \n",
    "print(len(eng_corpus))\n",
    "print(len(kor_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "trained-vault",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:34:55.973556Z",
     "start_time": "2021-05-04T10:34:55.967994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean : 그들의 해악적 유머는 청중들이 느낄 수 있는 많은 얘기를 하며 그들에게 즐거움을 주고 있다 .\n",
      "English : their biting humor is something to which many in their audience can relate .\n"
     ]
    }
   ],
   "source": [
    "print(\"Korean :\", kor_corpus[100])   \n",
    "print(\"English :\", eng_corpus[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-ecology",
   "metadata": {},
   "source": [
    "## 데이터 토큰화 : Sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acute-annual",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:44:43.586715Z",
     "start_time": "2021-05-04T10:44:43.582134Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"ko\",\n",
    "                       pad_id=0,\n",
    "                       bos_id=1,\n",
    "                       eos_id=2,\n",
    "                       unk_id=3):\n",
    "\n",
    "    model_name = 'spm_GD10_'+lang\n",
    "\n",
    "    # 모델 생성 경로\n",
    "    temp_file = os.getenv(\n",
    "        'HOME')+'/aiffel/transformer/data/spm/GD10_'+lang+'.ko'\n",
    "\n",
    "    # 함수 파라미터 lang의 단어를 적용한 파일 코퍼스 파일 만듬\n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:   # lang corpus를 활용합니다.\n",
    "            f.write(str(row) + '\\n')\n",
    "\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '\\\n",
    "        --input={} \\\n",
    "        --model_prefix={} \\\n",
    "        --vocab_size={} \\\n",
    "        --pad_id={} \\\n",
    "        --bos_id={} \\\n",
    "        --eos_id={} \\\n",
    "        --unk_id={}'.format(temp_file, model_name, vocab_size, pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "    # 위 Train에서  --model_type = 'unigram'이 디폴트 적용되어 있습니다. --model_type = 'bpe' 로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(model_name+'.model')\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aerial-brazilian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:44:58.160449Z",
     "start_time": "2021-05-04T10:44:44.418510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-porter",
   "metadata": {},
   "source": [
    "4) 토크나이저를 활용해 __토큰의 길이가 50 이하__인 데이터를 선별하여 src_corpus 와 tgt_corpus 를 각각 구축하고, 텐서 enc_train 과 dec_train 으로 변환하세요! (❗모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-highland",
   "metadata": {},
   "source": [
    "## 토크나이징 & 텐서 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "given-income",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:50:32.797034Z",
     "start_time": "2021-05-04T10:50:28.271916Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj24/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fa14357259419b8c89823d7b5dc226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67942\n",
      "67942\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
    "for idx in tqdm_notebook(range(len(kor_corpus))):\n",
    "    token_ko = ko_tokenizer.encode_as_ids(kor_corpus[idx])\n",
    "    token_en = en_tokenizer.encode_as_ids(eng_corpus[idx])\n",
    "               \n",
    "    if len(token_ko) <= 50  and len(token_en) <= 50:\n",
    "        src_corpus.append(token_ko)\n",
    "        tgt_corpus.append(token_en)\n",
    "\n",
    "print(len(src_corpus))\n",
    "print(len(tgt_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "separate-march",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:50:37.870351Z",
     "start_time": "2021-05-04T10:50:37.864860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean : [16, 7, 570, 337, 1550, 26, 1348, 3064, 10, 564, 21, 8823, 6, 1639, 5]\n",
      "English : [1, 30, 1354, 9, 265, 28, 11, 1199, 120, 18, 3964, 377, 47, 290, 9, 71, 11, 3821, 12, 5, 119, 14, 4138, 10, 5, 246, 6, 1277, 18, 3713, 34, 5, 3221, 7, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Korean :\", src_corpus[100])   \n",
    "print(\"English :\", tgt_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "flush-temperature",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:50:43.830856Z",
     "start_time": "2021-05-04T10:50:43.214121Z"
    }
   },
   "outputs": [],
   "source": [
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "familiar-investing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:50:45.708705Z",
     "start_time": "2021-05-04T10:50:45.702106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67942, 50)\n",
      "(67942, 50)\n"
     ]
    }
   ],
   "source": [
    "print(enc_train.shape)\n",
    "print(dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-tulsa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-vitamin",
   "metadata": {},
   "source": [
    "# tep 3. 모델 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-senate",
   "metadata": {},
   "source": [
    "오늘 배운 내용을 활용해서 Transformer 모델을 설계해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-chosen",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "direct-latitude",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:52:25.227797Z",
     "start_time": "2021-05-04T10:52:25.222765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-gibson",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "macro-lloyd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:53:43.727106Z",
     "start_time": "2021-05-04T10:53:43.716417Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-roller",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "royal-matter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:54:22.290229Z",
     "start_time": "2021-05-04T10:54:22.285622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-palestinian",
   "metadata": {},
   "source": [
    "## Encoder 레이어 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "delayed-muscle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:55:21.021112Z",
     "start_time": "2021-05-04T10:55:21.013205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-louisville",
   "metadata": {},
   "source": [
    "## Decoder 레이어 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "consistent-width",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T10:55:47.443620Z",
     "start_time": "2021-05-04T10:55:47.434268Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-interpretation",
   "metadata": {},
   "source": [
    "## Encoder 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "velvet-domestic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:25:19.763955Z",
     "start_time": "2021-05-04T12:25:19.758633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-limit",
   "metadata": {},
   "source": [
    "## Decoder 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "disciplinary-calgary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:25:20.711117Z",
     "start_time": "2021-05-04T12:25:20.706691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-pride",
   "metadata": {},
   "source": [
    "## Transformer 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "opposite-catch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:25:21.570699Z",
     "start_time": "2021-05-04T12:25:21.564118Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-murray",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "psychological-defendant",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:25:22.434792Z",
     "start_time": "2021-05-04T12:25:22.429117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-destiny",
   "metadata": {},
   "source": [
    "## LearningRateSchedule 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "systematic-frank",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:25:23.289501Z",
     "start_time": "2021-05-04T12:25:23.285934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-matrix",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-click",
   "metadata": {},
   "source": [
    "# Step 4. 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-maker",
   "metadata": {},
   "source": [
    "앞서 필요한 것들을 모두 정의했기 때문에 우리는 훈련만 하면 됩니다! 아래 과정을 차근차근 따라가며 모델을 훈련하고, __예문에 대한 멋진 번역__을 제출하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-repeat",
   "metadata": {},
   "source": [
    "## Transformer 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-camel",
   "metadata": {},
   "source": [
    "1. __2 Layer__를 가지는 Transformer를 선언하세요.\n",
    "(하이퍼파라미터는 자유롭게 조절합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "brazilian-coast",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:25:27.154712Z",
     "start_time": "2021-05-04T12:25:27.045452Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers = 2,\n",
    "    d_model = 512,\n",
    "    n_heads = 8,\n",
    "    d_ff = 2048,\n",
    "    src_vocab_size=20000,\n",
    "    tgt_vocab_size=20000,\n",
    "    pos_len=50,\n",
    "    dropout = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-subject",
   "metadata": {},
   "source": [
    "## Optimizertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-france",
   "metadata": {},
   "source": [
    "2. 논문에서 사용한 것과 동일한 __Learning Rate Scheduler__를 선언하고, 이를 포함하는 __Adam Optimizer__를 선언하세요. (Optimizer의 파라미터 역시 논문과 동일하게 설정합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "swiss-sheriff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:26:06.338917Z",
     "start_time": "2021-05-04T12:26:06.333022Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-09, \n",
    "    name='Adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-spokesman",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-ivory",
   "metadata": {},
   "source": [
    "3. __Loss 함수를 정의__하세요.\n",
    "Sequence-to-sequence 모델에서 사용했던 Loss와 유사하되, __Masking 되지 않은 입력의 개수로 Scaling__하는 과정을 추가합니다. (트랜스포머가 모든 입력에 대한 Loss를 한 번에 구하기 때문입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "earned-correspondence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:29:58.761117Z",
     "start_time": "2021-05-04T12:29:58.757751Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-capitol",
   "metadata": {},
   "source": [
    "## Train Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-absorption",
   "metadata": {},
   "source": [
    "4. __train_step 함수__를 정의하세요.\n",
    "__입력 데이터에 알맞은 Mask를 생성__하고, 이를 모델에 전달하여 연산에서 사용할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "immune-belfast",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:31:15.009574Z",
     "start_time": "2021-05-04T12:31:15.002875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-assignment",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-threshold",
   "metadata": {},
   "source": [
    "5. __학습을 진행__합니다.\n",
    "__매 Epoch 마다 제시된 예문에 대한 번역을 생성__하고, 멋진 번역이 생성되면 그때의 __하이퍼파라미터와 생성된 번역을 제출__하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-supervisor",
   "metadata": {},
   "source": [
    "__예문__\n",
    "``` python\n",
    "1. 오바마는 대통령이다.\n",
    "2. 시민들은 도시 속에 산다.\n",
    "3. 커피는 필요 없다.\n",
    "4. 일곱 명의 사망자가 발생했다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-synthesis",
   "metadata": {},
   "source": [
    "__결과(output)__\n",
    "``` python\n",
    "Translations\n",
    "> 1. obama is the president elect .\n",
    "> 2. they are in the city .\n",
    "> 3. they don t need to be a lot of drink .\n",
    "> 4. seven other people have been killed in the attacks .\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 2\n",
    "> d_model: 512\n",
    "> n_heads: 8\n",
    "> d_ff: 2048\n",
    "> dropout: 0.3\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 4000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-resource",
   "metadata": {},
   "source": [
    "번역 생성에는 아래 소스를 사용하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "criminal-escape",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:32:13.082950Z",
     "start_time": "2021-05-04T12:32:13.072996Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "stopped-devices",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:32:17.904909Z",
     "start_time": "2021-05-04T12:32:17.898203Z"
    }
   },
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "palestinian-spending",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T12:32:45.644750Z",
     "start_time": "2021-05-04T12:32:45.639775Z"
    }
   },
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-jumping",
   "metadata": {},
   "source": [
    "translate() 함수의 plot_attention 변수를 True 로 주면 번역 결과에 대한 Attention Map을 시각화 해볼 수 있습니다.\n",
    "\n",
    "💡 이번 프로젝트에서 제시한 예문은 Seq2seq으로 번역기 만들기의 예문과 동일합니다. Seq2seq과 Transformer로 만든 두 번역기의 성능을 하이퍼파라미터를 조정 등 다양한 연구해보시면 학습에 도움이 되실 거예요!\n",
    "\n",
    "마지막으로, 학습의 전 과정을 구현한 코드를 첨부합니다. 구현과정에 참고해 주세요. 수고하셨습니다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-football",
   "metadata": {},
   "source": [
    "``` python\n",
    "# 학습\n",
    "\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "examples = [\n",
    "            \"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "examples = [\n",
    "            \"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-pricing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-blind",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-mixture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-writing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-perspective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-ownership",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-monte",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-offense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-delay",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-decline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-estimate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-surrey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-rebate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-robin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-capture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-snowboard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-january",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-accounting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-scholarship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-forwarding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-screw",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-columbia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-blackberry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-wireless",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-space",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-quarterly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-campbell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-partnership",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-adult",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-estate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-childhood",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-hawaii",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-landscape",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-individual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-chair",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-giant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-constant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-boutique",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-comment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-pressure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-yeast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-research",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-kentucky",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-grove",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
